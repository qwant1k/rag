"""
Ingestion –ø–∞–π–ø–ª–∞–π–Ω ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ChromaDB.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: PDF (PyMuPDF), DOCX (python-docx), DOC (pywin32 COM), TXT.
"""

import logging
import hashlib
import re
from datetime import datetime
from pathlib import Path
from typing import Optional

import fitz  # PyMuPDF
from docx import Document as DocxDocument
from docx.oxml.table import CT_Tbl
from docx.oxml.text.paragraph import CT_P
from docx.table import Table
from docx.text.paragraph import Paragraph
from PIL import Image
import pytesseract
import io
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

from backend.config import (
    DOCUMENTS_DIR,
    CHROMA_DB_DIR,
    EMBEDDING_MODEL,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    CHROMA_COLLECTION_NAME,
    TESSERACT_CMD,
)

logger = logging.getLogger("rag-chatbot")

# –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ tesseract.exe –¥–ª—è Windows
pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD


def normalize_text(text: str) -> str:
    """
    –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π:
    - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –¥–µ—Ñ–∏—Å,
    - —É–¥–∞–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö unicode-—Å–∏–º–≤–æ–ª–æ–≤,
    - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤.
    """
    if not text:
        return ""

    cleaned = text.replace("\r\n", "\n").replace("\r", "\n")
    cleaned = cleaned.replace("\u00ad", "")  
    cleaned = cleaned.replace("\xa0", " ")   
    cleaned = re.sub(r"([A-Za-z–ê-–Ø–∞-—è–Å—ë])-\n([A-Za-z–ê-–Ø–∞-—è–Å—ë])", r"\1\2", cleaned)
    cleaned = re.sub(r"[ \t]+", " ", cleaned)
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned)
    return cleaned.strip()


def should_use_ocr(text: str) -> bool:
    """
    –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö PDF —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –µ—Å—Ç—å, –Ω–æ –æ–Ω "–ø—É—Å—Ç–æ–π" –¥–ª—è RAG
    (–º–∞–ª–æ –±—É–∫–≤/–º—É—Å–æ—Ä). –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø—Ä–æ–±—É–µ–º OCR.
    """
    if not text or not text.strip():
        return True

    letters = sum(1 for ch in text if ch.isalpha())
    return letters < 40


def iter_docx_blocks(doc: DocxDocument):
    """–ò—Ç–µ—Ä–∞—Ç–æ—Ä –ø–æ –±–ª–æ–∫–∞–º DOCX –≤ –ø–æ—Ä—è–¥–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: –∞–±–∑–∞—Ü—ã –∏ —Ç–∞–±–ª–∏—Ü—ã."""
    for child in doc.element.body.iterchildren():
        if isinstance(child, CT_P):
            yield Paragraph(child, doc)
        elif isinstance(child, CT_Tbl):
            yield Table(child, doc)


def get_relative_source(file_path: Path) -> str:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å —Ñ–∞–π–ª–∞ –æ—Ç –ø–∞–ø–∫–∏ DOCUMENTS_DIR.
    –ù–∞–ø—Ä–∏–º–µ—Ä: documents/–¥–æ–≥–æ–≤–æ—Ä—ã/2024/–¥–æ–≥–æ–≤–æ—Ä1.pdf ‚Üí –¥–æ–≥–æ–≤–æ—Ä—ã/2024/–¥–æ–≥–æ–≤–æ—Ä1.pdf
    –ï—Å–ª–∏ —Ñ–∞–π–ª –≤–Ω–µ DOCUMENTS_DIR ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ –∏–º—è —Ñ–∞–π–ª–∞.
    """
    try:
        return str(file_path.relative_to(DOCUMENTS_DIR)).replace("\\", "/")
    except ValueError:
        return file_path.name


def get_embeddings() -> HuggingFaceEmbeddings:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (all-MiniLM-L6-v2)."""
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBEDDING_MODEL}")
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )


def get_vectorstore(embeddings: Optional[HuggingFaceEmbeddings] = None) -> Chroma:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ ChromaDB vectorstore."""
    if embeddings is None:
        embeddings = get_embeddings()
    return Chroma(
        collection_name=CHROMA_COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=str(CHROMA_DB_DIR),
    )


# === –ü–∞—Ä—Å–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ===

def ocr_page(page) -> str:
    """OCR –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF —á–µ—Ä–µ–∑ pytesseract (–¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)."""
    try:
        pix = page.get_pixmap(dpi=300)
        img_bytes = pix.tobytes("png")
        image = Image.open(io.BytesIO(img_bytes))
        text = pytesseract.image_to_string(image, lang="rus+eng+kaz")
        return text.strip()
    except Exception as e:
        logger.warning(f"OCR –æ—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {e}")
        return ""


def parse_pdf(file_path: Path) -> list[Document]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ PDF —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ PyMuPDF.
    –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è OCR —á–µ—Ä–µ–∑ pytesseract.
    """
    documents = []
    ocr_used = False
    try:
        pdf = fitz.open(str(file_path))
        for page_num in range(len(pdf)):
            page = pdf[page_num]
            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π
            text = normalize_text(page.get_text("text"))
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç/–º—É—Å–æ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî –ø—Ä–æ–±—É–µ–º OCR
            if should_use_ocr(text):
                ocr_text = normalize_text(ocr_page(page))
                if ocr_text:
                    text = ocr_text
                    ocr_used = True
            if text:
                documents.append(
                    Document(
                        page_content=text,
                        metadata={
                            "source": get_relative_source(file_path),
                            "page": page_num + 1,
                            "upload_date": datetime.now().isoformat(),
                        },
                    )
                )
        pdf.close()
        method = "OCR" if ocr_used else "—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π"
        logger.info(f"PDF '{get_relative_source(file_path)}': –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(documents)} —Å—Ç—Ä–∞–Ω–∏—Ü ({method})")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ PDF '{get_relative_source(file_path)}': {e}")
    return documents


def parse_docx(file_path: Path) -> list[Document]:
    """–ü–∞—Ä—Å–∏–Ω–≥ DOCX —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ python-docx. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ Document."""
    documents = []
    try:
        doc = DocxDocument(str(file_path))
        blocks: list[str] = []

        for block in iter_docx_blocks(doc):
            if isinstance(block, Paragraph):
                text = normalize_text(block.text)
                if text:
                    blocks.append(text)
            elif isinstance(block, Table):
                for row in block.rows:
                    cells = [normalize_text(cell.text) for cell in row.cells]
                    cells = [cell for cell in cells if cell]
                    if cells:
                        blocks.append(" | ".join(cells))

        full_text = "\n".join(blocks)
        if full_text.strip():
            documents.append(
                Document(
                    page_content=full_text,
                    metadata={
                        "source": get_relative_source(file_path),
                        "page": 1,
                        "upload_date": datetime.now().isoformat(),
                    },
                )
            )
        logger.info(f"DOCX '{get_relative_source(file_path)}': –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ DOCX '{get_relative_source(file_path)}': {e}")
    return documents


def parse_txt(file_path: Path) -> list[Document]:
    """–ü–∞—Ä—Å–∏–Ω–≥ TXT —Ñ–∞–π–ª–∞. –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É UTF-8."""
    documents = []
    try:
        text = normalize_text(file_path.read_text(encoding="utf-8"))
        if text.strip():
            documents.append(
                Document(
                    page_content=text,
                    metadata={
                        "source": get_relative_source(file_path),
                        "page": 1,
                        "upload_date": datetime.now().isoformat(),
                    },
                )
            )
        logger.info(f"TXT '{get_relative_source(file_path)}': –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    except UnicodeDecodeError:
        # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π cp1251 (—á–∞—Å—Ç–∞—è –Ω–∞ Windows)
        try:
            text = normalize_text(file_path.read_text(encoding="cp1251"))
            if text.strip():
                documents.append(
                    Document(
                        page_content=text,
                        metadata={
                            "source": get_relative_source(file_path),
                            "page": 1,
                            "upload_date": datetime.now().isoformat(),
                        },
                    )
                )
            logger.info(f"TXT '{get_relative_source(file_path)}': –ø—Ä–æ—á–∏—Ç–∞–Ω –≤ cp1251, {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ TXT '{get_relative_source(file_path)}': {e}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ TXT '{get_relative_source(file_path)}': {e}")
    return documents


def parse_doc(file_path: Path) -> list[Document]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ DOC —Ñ–∞–π–ª–∞ (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç Word) —á–µ—Ä–µ–∑ pywin32 COM.
    –¢—Ä–µ–±—É–µ—Ç—Å—è Windows —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º MS Word.
    """
    documents = []
    try:
        import pythoncom
        import win32com.client
        pythoncom.CoInitialize()
        try:
            word = win32com.client.Dispatch("Word.Application")
            word.Visible = False
            doc = word.Documents.Open(str(file_path.resolve()))
            text = doc.Content.Text
            doc.Close(False)
            word.Quit()
        finally:
            pythoncom.CoUninitialize()

        if text.strip():
            documents.append(
                Document(
                    page_content=normalize_text(text),
                    metadata={
                        "source": get_relative_source(file_path),
                        "page": 1,
                        "upload_date": datetime.now().isoformat(),
                    },
                )
            )
        logger.info(f"DOC '{get_relative_source(file_path)}': –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    except ImportError:
        logger.error("–î–ª—è —á—Ç–µ–Ω–∏—è .doc —Ñ–∞–π–ª–æ–≤ —Ç—Ä–µ–±—É–µ—Ç—Å—è pywin32: pip install pywin32")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ DOC '{get_relative_source(file_path)}': {e}")
    return documents


def parse_file(file_path: Path) -> list[Document]:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä ‚Äî –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–π –ø–∞—Ä—Å–µ—Ä."""
    suffix = file_path.suffix.lower()
    parsers = {
        ".pdf": parse_pdf,
        ".docx": parse_docx,
        ".doc": parse_doc,
        ".txt": parse_txt,
    }
    parser = parsers.get(suffix)
    if parser is None:
        logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path.name}")
        return []
    return parser(file_path)


def split_documents(documents: list[Document]) -> list[Document]:
    """–†–∞–∑–±–∏–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ": ", " ", ""],
    )
    chunks = text_splitter.split_documents(documents)
    logger.info(f"–†–∞–∑–±–∏–≤–∫–∞: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
    return chunks


def generate_chunk_id(chunk: Document, index: int) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
    content = f"{chunk.metadata.get('source', '')}_{chunk.metadata.get('page', '')}_{index}_{chunk.page_content[:100]}"
    return hashlib.md5(content.encode("utf-8")).hexdigest()


def delete_document_from_db(filename: str, vectorstore: Optional[Chroma] = None) -> int:
    """
    –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ ChromaDB –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    """
    if vectorstore is None:
        vectorstore = get_vectorstore()

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    collection = vectorstore._collection
    results = collection.get(where={"source": filename})

    if results and results["ids"]:
        count = len(results["ids"])
        collection.delete(ids=results["ids"])
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ {count} —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{filename}' –∏–∑ ChromaDB")
        return count

    logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç '{filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ ChromaDB")
    return 0


def ingest_file(file_path: Path, vectorstore: Optional[Chroma] = None) -> int:
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≥—Ä—É–∑–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞:
    1. –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞
    2. –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
    3. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)
    4. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ ChromaDB
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    """
    if vectorstore is None:
        vectorstore = get_vectorstore()

    source_name = get_relative_source(file_path)
    logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {source_name}")

    # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥
    documents = parse_file(file_path)
    if not documents:
        logger.warning(f"–§–∞–π–ª '{source_name}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω")
        return 0

    # –®–∞–≥ 2: –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
    chunks = split_documents(documents)
    if not chunks:
        logger.warning(f"–§–∞–π–ª '{source_name}': –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–µ—Ç —á–∞–Ω–∫–æ–≤")
        return 0

    # –®–∞–≥ 3: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è ‚Äî —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ (–ø–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º—É –ø—É—Ç–∏)
    delete_document_from_db(source_name, vectorstore)

    # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ ChromaDB
    ids = [generate_chunk_id(chunk, i) for i, chunk in enumerate(chunks)]
    vectorstore.add_documents(documents=chunks, ids=ids)

    logger.info(f"‚úÖ –§–∞–π–ª '{source_name}': –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ ChromaDB")
    return len(chunks)


def ingest_directory(directory: Optional[Path] = None) -> dict:
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞–ø–æ–∫.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Path.rglob() –¥–ª—è –æ–±—Ö–æ–¥–∞ –Ω–∞ –ª—é–±—É—é –≥–ª—É–±–∏–Ω—É –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π_–ø—É—Ç—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—á–∞–Ω–∫–æ–≤}.
    """
    if directory is None:
        directory = DOCUMENTS_DIR

    if not directory.exists():
        logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {directory}")
        return {}

    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è ‚Äî —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ rglob
    extensions = {".pdf", ".docx", ".doc", ".txt"}
    files = [
        f for f in directory.rglob("*")
        if f.is_file()
        and f.suffix.lower() in extensions
        and not f.name.startswith("~$")  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã Word
    ]

    if not files:
        logger.warning(f"–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ '{directory}' (–≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ) –Ω–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤")
        return {}

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥)")

    # –°–æ–∑–¥–∞—ë–º vectorstore –∏ embeddings –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    embeddings = get_embeddings()
    vectorstore = get_vectorstore(embeddings)

    results = {}
    for file_path in sorted(files):
        source_name = get_relative_source(file_path)
        try:
            count = ingest_file(file_path, vectorstore)
            results[source_name] = count
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ '{source_name}': {e}")
            results[source_name] = 0

    total_chunks = sum(results.values())
    logger.info(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(results)} —Ñ–∞–π–ª–æ–≤, {total_chunks} —á–∞–Ω–∫–æ–≤ –≤—Å–µ–≥–æ")
    return results


def get_indexed_documents() -> list[dict]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ ChromaDB.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–∂–¥–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ.
    """
    try:
        vectorstore = get_vectorstore()
        collection = vectorstore._collection
        all_data = collection.get(include=["metadatas"])

        if not all_data or not all_data["metadatas"]:
            return []

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        doc_info = {}
        for metadata in all_data["metadatas"]:
            source = metadata.get("source", "unknown")
            if source not in doc_info:
                doc_info[source] = {
                    "filename": source,
                    "chunks_count": 0,
                    "pages": set(),
                    "upload_date": metadata.get("upload_date", ""),
                }
            doc_info[source]["chunks_count"] += 1
            page = metadata.get("page")
            if page:
                doc_info[source]["pages"].add(page)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º set –≤ sorted list –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        result = []
        for info in doc_info.values():
            info["pages"] = sorted(info["pages"])
            result.append(info)

        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        return []


# === –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ ===
if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("–ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ /documents")
    logger.info("=" * 60)

    results = ingest_directory()

    if results:
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:")
        print("-" * 40)
        for filename, count in results.items():
            status = "‚úÖ" if count > 0 else "‚ùå"
            print(f"  {status} {filename}: {count} —á–∞–Ω–∫–æ–≤")
        print("-" * 40)
        print(f"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {sum(results.values())}")
    else:
        print("\n‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. –ü–æ–ª–æ–∂–∏—Ç–µ PDF, DOCX –∏–ª–∏ TXT —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É /documents")
